{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"sector_chosen_11(v4).ipynb","provenance":[],"collapsed_sections":["MP-lWLHmOsmb","BlZGTXjoO2ng"],"authorship_tag":"ABX9TyNGAUBK0pahyBU6cy/dkTCK"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"e8VVftxrqIKx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624389947883,"user_tz":-60,"elapsed":47573,"user":{"displayName":"川崎聖馬","photoUrl":"","userId":"10711466813477703476"}},"outputId":"5f3eb955-8fbf-4f29-944a-d40fc1de592c"},"source":["from google.colab import drive\n","import os\n","drive.mount('/content/drive/')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ExdP8CvmASam","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624389948589,"user_tz":-60,"elapsed":736,"user":{"displayName":"川崎聖馬","photoUrl":"","userId":"10711466813477703476"}},"outputId":"cf22c43e-39e6-4dca-98c3-b1ccb679c518"},"source":["!pwd\n","%cd /content/drive/MyDrive/LSTM project/LSTM_Project/sector_analysis(food drug)/2016-2019(before covid)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content\n","/content/drive/MyDrive/LSTM project/LSTM_Project/sector_analysis(food drug)/2016-2019(before covid)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6HKnioKLBtBw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624392877701,"user_tz":-60,"elapsed":282,"user":{"displayName":"川崎聖馬","photoUrl":"","userId":"10711466813477703476"}},"outputId":"5312b2a5-a009-450f-9efe-b2f4eabf29cd"},"source":["# sector-> Food & Drug store(2016/01/01~2019/12/31) \n","import pandas as pd\n","df1=pd.read_csv(\"SVNDY.csv\") \n","df2=pd.read_csv(\"WBA.csv\")  \n","df3=pd.read_csv(\"MTRAF.csv\")\n","df4=pd.read_csv(\"COOP.csv\") \n","df5=pd.read_csv(\"JSAIY.csv\")  \n","df6=pd.read_csv(\"ADRNY.csv\")  \n","df7=pd.read_csv(\"TSCDY.csv\")  \n","df8=pd.read_csv(\"KR.csv\")    \n","df9=pd.read_csv(\"AGHC.csv\")  \n","df10=pd.read_csv(\"CRRFY.csv\") \n","df11=pd.read_csv(\"WNGRF.csv\") \n","\n","print(df1.shape, df2.shape, df3.shape, df4.shape, df5.shape, df6.shape, df7.shape, df8.shape, df9.shape, df10.shape, df11.shape)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["(1006, 8) (1006, 8) (1006, 8) (1006, 8) (1006, 8) (1006, 8) (1006, 8) (1006, 8) (1006, 8) (1006, 8) (1006, 8)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MP-lWLHmOsmb"},"source":["# Make models (train)"]},{"cell_type":"code","metadata":{"id":"OlZQw47dNTBB","executionInfo":{"status":"ok","timestamp":1624394477080,"user_tz":-60,"elapsed":968,"user":{"displayName":"川崎聖馬","photoUrl":"","userId":"10711466813477703476"}}},"source":["from keras.layers import *\n","def make_models(c, nl, v, V=False, output_c=['High'], output_i=[2], outputs_company=1,\n","                cross_validation=True, train_size=754, time_step=60, epochs=30, units=50): \n","  import warnings\n","  warnings.simplefilter('ignore')\n","  import math\n","  import seaborn as sns\n","  import matplotlib.pyplot as plt\n","  import keras\n","  import pandas as pd\n","  import numpy as np\n","  from keras.models import Sequential\n","  from keras.layers import Dense\n","  from keras.layers import LSTM\n","  from keras.layers import Dropout\n","  from sklearn.preprocessing import MinMaxScaler\n","  from sklearn.preprocessing import StandardScaler\n","  from sklearn.metrics import mean_squared_error\n","  from sklearn.metrics import mean_absolute_error\n","  from sklearn.model_selection import train_test_split\n","  from keras.callbacks import EarlyStopping\n","  \"\"\"\n","  items={\"Date\":0, \"Open\":1, \"High\":2, \"Low\":3, \"Close\":4, \"Adj Close\":5, \"Volume\":6}\n","  v=[1, 2, 3, 4, 6]  \n","  output_c=['High']\n","  output_i=[2] 　\"\"\"\n","  \n","  print(f\"[Arguments]\") \n","  print(f\"num_company={len(nl)}社 {nl}\")\n","  print(f\"input_item={v}, outout_item={output_i}, output_category={output_c}\")\n","  print(f\"train_size={train_size}, time_step={time_step}, num_epoch={epochs}, num_units(neurons)={units}\")\n","  print(\"\")\n","\n","  print(\"~~Make models~~\")\n","  #1. split train and test data\n","  train_data=[]\n","  test_data=[]\n","  for i in range(len(c)):\n","    train_data.append(c[i].iloc[: train_size, v].values)\n","    test_data.append(c[i].iloc[train_size:, v].values)\n","  print(f\"train_data={np.array(train_data).shape}, test_data={np.array(test_data).shape}\")\n","\n","  #2. train_data->normalise depending on number of inputs (only the volume is processed separately as the scale is different)\n","  #3. make X_train, y_train flexibly\n","  X_train=[]\n","  y_trains=[]\n","  \n","  if len(output_i) != 1:\n","    # print(f\"output>=2, inputs={v}, outputs={output_c}\")\n","    training_set_others=np.hstack((data[:,:-1] for data in train_data))\n","    training_set_volume=np.hstack((data[:,-1:].reshape(-1, 1) for data in train_data))\n","\n","    ss_others=[]\n","    for i in range(training_set_others.shape[1]):\n","      ss1=StandardScaler()\n","      ss_others.append(ss1.fit_transform(training_set_others[:,i].reshape(-1, 1)))\n","    ss_others=np.hstack(ss_others)\n","\n","    ss2=StandardScaler()\n","    ss_v=ss2.fit_transform(training_set_volume)\n","    concat=np.hstack((ss_others, ss_v))\n","    # print(f\"concat={concat.shape}\")\n","\n","    #ind=output用のvariablesのindexを抜き取る=>個別に学習モデルを作成するため\n","    if 6 in output_i: \n","      # print(\"Volume含む\")\n","      ind = []\n","      for i in range(len(output_i)):\n","        if output_i[i] != 6:\n","          ind+=[j for j in range(v.index(output_i[i]), len(c)*(len(v)-1), len(v)-1)] \n","        else:\n","          ind+=[k for k in range(len(c)*(len(v)-1), len(c)*len(v))]\n","                              \n","      # print(f\"len(ind)={len(ind)}, ind={ind}\")\n","      for l in range(time_step, train_size):\n","        X_train.append(concat[l-time_step: l, :])\n","        for m in range(len(ind)):\n","          y_trains.append(concat[l, ind[m]])\n","      print(len(X_train), len(y_trains))\n","\n","    else:\n","      # print(\"Volume含まない\")\n","      ind=[]\n","      for i in range(len(output_i)):\n","        ind+=[j for j in range(v.index(output_i[i]), len(c)*(len(v)), len(v))] \n","      # print(f\"num_of_companies={len(c)}, ind={ind}\")\n","      \n","      for l in range(time_step, train_size):\n","        X_train.append(concat[l-time_step: l, :])\n","        for m in range(len(ind)):\n","          y_trains.append(concat[l, ind[m]])\n","      print(len(X_train), len(y_trains))\n","\n","  else: #outputが１つの時\n","    if V:\n","      # print(\"Here check\")\n","      # print(f\"output=1, inputs={v}, outputs={output_c}\")\n","\n","      training_set_volume=np.hstack((data[:,-1].reshape(-1, 1) for data in train_data))\n","      ss2=StandardScaler()\n","      ss_v=ss2.fit_transform(training_set_volume)\n","\n","      # print(f\"concat={ss_v.shape}\")\n","\n","      ind = [id for id in range(len(c))]\n","      # print(f\"num_of_companies={len(c)}, ind={ind}\")\n","\n","      for j in range(time_step, train_size):\n","        X_train.append(ss_v[j-time_step: j, :])\n","        for k in range(len(ind)):\n","          y_trains.append(ss_v[j, ind[k]])\n","\n","    else: # volume含まれていない時\n","      # print(\"volule以外かつinputとoutputは1つ\")\n","      training_set_all=np.hstack(np.array(train_data))\n","      ss2=StandardScaler()\n","      ss_others=ss2.fit_transform(training_set_all)\n","      concat=ss_others\n","\n","      ind = [id for id in range(len(c))]\n","      # print(f\"num_of_companies={len(c)}, ind={ind}\")\n","\n","      for j in range(time_step, train_size):\n","        X_train.append(concat[j-time_step: j, :])\n","        for k in range(len(ind)):\n","          y_trains.append(concat[j, ind[k]])\n","\n","  X_train=np.array(X_train)\n","  y_trains=np.array(y_trains)\n","  print(f\"X_train={X_train.shape}, y_trains={y_trains.shape}\")\n","  # print(\"finish making X_train and y_trains\")\n","  #3.train models(numbers of companies)\n","\n","  y_trains2=[]\n","  \n","  if len(output_i) != 1:\n","    for x in range(len(c)*len(output_i)):\n","      x_tr=np.array([y_trains[i] for i in range(x, len(y_trains), len(c)*len(output_i))])\n","      y_trains2.append(x_tr)\n","\n","  else: # len(output_i)==1\n","    for y in range(len(c)):\n","      y_tr=np.array([y_trains[i] for i in range(y, len(y_trains), len(c))])\n","      y_trains2.append(y_tr)\n","\n","  y_trains2=np.array(y_trains2)\n","  print(f\"y_trains2={y_trains2.shape}\")\n","\n","  model_lists = []\n","  \n","  for l in range(outputs_company):\n","    print(f\"{l+1}/{outputs_company}\")\n","    model = Sequential()\n","    #Adding the first LSTM layer and some Dropout regularisation\n","    # ここのinput_shapeで入力次元を設定する\n","    model.add(LSTM(units = units, return_sequences = True, input_shape=(X_train.shape[1], X_train.shape[2] )))\n","    model.add(Dropout(0.2))\n","    # Adding a second LSTM layer and some Dropout regularisation\n","    model.add(LSTM(units = units, return_sequences = True))\n","    model.add(Dropout(0.2))\n","    # Adding a third LSTM layer and some Dropout regularisation\n","    model.add(LSTM(units = units, return_sequences = True))\n","    model.add(Dropout(0.2))\n","    # Adding a fourth LSTM layer and some Dropout regularisation\n","    model.add(LSTM(units = units))\n","    model.add(Dropout(0.2))\n","    # Adding the output layer\n","    model.add(Dense(units = 1))\n","\n","    # Compiling the RNN\n","    model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n","\n","    # Fitting the RNN to the Training set\n","    model.fit(X_train, y_trains2[l], epochs=epochs, batch_size=32)\n","    model_lists.append(model)\n","    print(\"\")\n","  return model_lists, y_trains2[:outputs_company]"],"execution_count":36,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BlZGTXjoO2ng"},"source":["# Make predictions (test)\n"]},{"cell_type":"code","metadata":{"id":"oHBGI9PhNS0V","executionInfo":{"status":"ok","timestamp":1624394344347,"user_tz":-60,"elapsed":550,"user":{"displayName":"川崎聖馬","photoUrl":"","userId":"10711466813477703476"}}},"source":["# Make predictions\n","def make_preds(c, nl, v, model_lists, y_trains2, V=False, output_c=['High'], output_i=[2], train_size=754, time_step=60):\n","  import warnings\n","  warnings.simplefilter('ignore')\n","  import math\n","  import seaborn as sns\n","  import matplotlib.pyplot as plt\n","  import keras\n","  import pandas as pd\n","  import numpy as np\n","  from keras.models import Sequential\n","  from keras.layers import Dense\n","  from keras.layers import LSTM\n","  from keras.layers import Dropout\n","  from sklearn.preprocessing import MinMaxScaler\n","  from sklearn.preprocessing import StandardScaler\n","  from sklearn.metrics import mean_squared_error\n","  from sklearn.metrics import mean_absolute_error\n","  from sklearn.model_selection import train_test_split\n","  from keras.callbacks import EarlyStopping\n","  \n","  print(\"~~Make predctions~~\")\n","  #4. Prepare test data and normalise and make predictions\n","  train_data=[]\n","  test_data=[]\n","  for i in range(len(c)):\n","    train_data.append(c[i].iloc[: train_size, v].values)\n","    test_data.append(c[i].iloc[train_size:, v].values)\n","  print(f\"train_data={np.array(train_data).shape}, test_data={np.array(test_data).shape}\")\n","\n","  inputs = []\n","  \n","  for m in range(len(c)): # ここではvで指定したvariablesだけのarrayに加工している\n","    inputs.append(c[m].iloc[:, v][len(c[m])-len(test_data[m])-time_step:].values)\n","  \n","  inputs = np.array(inputs)\n","  variables=[]\n","  ss=[]\n","  for i in range(len(c)):\n","    for j in range(len(v)): #11*5=55\n","      m=inputs[i, :, j].reshape(-1, 1) \n","      s=StandardScaler()\n","      t=s.fit_transform(m)\n","\n","      variables.append(t)\n","      ss.append(s)\n","  \n","  inputs2=np.hstack(variables)\n","  s1=StandardScaler()\n","  inputs2=s1.fit_transform(inputs2)\n","  \n","  X_test = []\n","  # use all components as imputs for testing\n","  for n in range(time_step, np.array(test_data).shape[1]+time_step):\n","    X_test.append(inputs2[n-time_step:n, :])\n","\n","  X_test = np.array(X_test)\n","  preds = [] #(11, 252) (252, 1)\n","  for o in range(len(y_trains2)):\n","    preds.append(np.array(model_lists[o].predict(X_test)))\n","  \n","  if len(output_i)==1:\n","    ind=[i for i in range(v.index(output_i[0]), len(c)*len(v), len(v))]\n","  else:\n","    ind=[]\n","    for i in range(len(output_i)):\n","      ind+=[j for j in range(v.index(output_i[i]), len(c)*len(v), len(v))]\n","\n","  for p in range(len(y_trains2)): #(252, 11)\n","    preds[p] = ss[ind[p]].inverse_transform(preds[p]) #修正必要\n","  preds = np.array(preds)\n","\n","  #5. Plot actual vs preds\n","  # df ={\"Date\":0, \"Open\":1, \"High\":2, \"Low\":3, \"Close\":4, \"Adj Close\":5, \"Volume\":6] \n","  actual_lists=[]\n","  for dd in range(len(output_i)):\n","    # for cc in range(len(c)):\n","    actual_lists+=[sc.iloc[train_size:,output_i[dd]] for sc in c]\n","  \n","  if len(output_i) > 1:\n","    R2=nl*len(output_i) # lists of companies\n","  else: R2=nl\n","  \n","  print(f\"actual lists={np.array(actual_lists).shape}, preds_list={np.array(preds).shape}\")\n","\n","  R=0\n","  r=[r for r in range(len(c)-1, len(R2), len(c))]\n","  print(r)\n","  for p in range(len(R2)):\n","    plt.figure(figsize=(10, 6)) \n","    plt.plot(c[0].loc[train_size:,'Date'], actual_lists[p], color='red', label='Real')\n","    plt.plot(c[0].loc[train_size:,'Date'], preds[p], color='blue', label='Predict')\n","    from sklearn.metrics import r2_score\n","\n","    plt.xticks(np.arange(0, 300, 30))\n","    # plt.title(f\"{}\")\n","    plt.xlabel('Time series')\n","    plt.ylabel('Stock Price (or Volume)')\n","    \n","    # use correlation\n","    print(f\"R2{R2[p]}={r2_score(actual_lists[p], preds[p].flatten())}\")\n"," \n","    R +=r2_score(actual_lists[p], preds[p].flatten())\n","    plt.legend()\n","    plt.show()\n","\n","    if p in r:\n","      print(f\"R2-{len(c)}companies_average={R/len(c)}\")\n","      R=0\n","      print(\"\")\n","\n","  #6. Make confusion matrix\n","\n","  columns = nl # labeling \n","  # Actual \n","  # print(\"Actual\")\n","  # act=[a for a in actual_lists]\n","  # Actual=pd.concat(act, axis=1)\n","  # Actual.columns = columns\n","  # plt.figure(figsize=(12, 10))\n","  # cmap=sns.diverging_palette(220, 20, as_cmap=True)\n","  # sns.heatmap(Actual.corr(method='spearman'), cmap=cmap, annot=True) \n","\n","  # Predictions\n","  for p in range(len(output_i)):\n","    print(f\"Preds_Heatmap={output_c[p]} correlation\")\n","    prd=[preds[p] for p in range(len(c)*p, len(c)+p*len(c))]\n","    # print(len(prd))\n","    Preds= pd.DataFrame(np.concatenate(prd, axis=1))\n","    Preds.columns = columns\n","    # print(Preds.columns)\n","    plt.figure(figsize=(8, 6))\n","    cmap=sns.diverging_palette(220, 20, as_cmap=True)\n","    sns.heatmap(Preds.corr(method='spearman'), cmap=cmap, annot=True) \n","\n","  # return preds[p].flatten()\n","\n","  \"\"\"Metro can be a special case, but the ohter companies in the same sector would have a sort of similarity. \"\"\""],"execution_count":33,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hffMn7pcBikR"},"source":["# 6月22日以降　\n"]},{"cell_type":"markdown","metadata":{"id":"kk-hvaVbNOxM"},"source":["## ALL in ALL Function"]},{"cell_type":"code","metadata":{"id":"6LYzJzIFbILE","executionInfo":{"status":"ok","timestamp":1624394347440,"user_tz":-60,"elapsed":6,"user":{"displayName":"川崎聖馬","photoUrl":"","userId":"10711466813477703476"}}},"source":["all_data=[df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, df11]\n","items={\"Date\":0, \"Open\":1, \"High\":2, \"Low\":3, \"Close\":4, \"Adj Close\":5, \"Volume\":6}\n","\n","def all_in_all(all_data, num_company=11, input_num=[2], output_category=['High'], output_num=[2], outputs_company=11, \n","               cross_validation=False, train_size=754, time_step=60, epochs=20, units=50):\n","\n","  company_list=[\"Seven & I Holdings (Japan)\", \"Walgreens Boots Alliance (USA)\", \"Metro (Canada)\", 'Coop Group (Switzerland)', 'J.Sainsbury(Britain)',\n","    \"Royal Ahold Delhaize(Netherlands)\", \"Tesco(Britain)\", \"Kroger(USA)\", \"AEON(Japan)\", \"Carrefour(France)\", \"George Weston(Canada)\"]\n","  \n","  all_data=all_data[:num_company]\n","  company_list=company_list[:num_company]\n","\n","  if 6 not in input_num: Volume=False\n","  else: Volume=True\n","\n","  # Make Models\n","  model_lists, y_trains = make_models(all_data, company_list, input_num, Volume, output_category, \n","                                      output_num, outputs_company, cross_validation, train_size, time_step, epochs, units)\n","  # Make Predictions               \n","  make_preds(all_data, company_list, input_num, model_lists, y_trains, Volume, output_category, output_num, train_size, time_step)"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"id":"79LdVjR-WaP6"},"source":["all_in_all(all_data, num_company=5, input_num=[2], output_category=['High'], output_num=[2], outputs_company=5,\n","           cross_validation=False, train_size=754+126, time_step=30, epochs=50, units=30)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pl6WPAuxXCzN"},"source":[""],"execution_count":null,"outputs":[]}]}